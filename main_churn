#!/usr/bin/python

#Import modules
import numpy as np
import matplotlib.pyplot as mpl
import pandas as pd
import time
import os
import csv
from sklearn import linear_model
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier as rfc

#Classes
class Evaluation:
	def __init__(self, truevals, predictvals):
		self.truevals = truevals
		self.predictvals = predictvals
	
		self.tp = 0	#Counter of true positives
		self.fp = 0	#Counter of false positives
		self.tn = 0	#Counter of true negatives
		self.fn = 0	#Counter of false negatives
	
		#Code to count false/true positives/negatives
		for i in range(0,len(self.truevals)):
			if self.truevals[i] == 1 and self.predictvals[i] == 1:
				self.tp += 1
			elif self.truevals[i] == 0 and self.predictvals[i] == 1:
				self.fp += 1
			elif self.truevals[i] == 0 and self.predictvals[i] == 0:
				self.tn += 1
			elif self.truevals[i] == 1 and self.predictvals[i] == 0:
				self.fn += 1

	def truepositives(self):
		return self.tp

	def precision(self):
		return float(self.tp) / float(self.tp + self.fp)

	def recall(self):
		return float(self.tp) / float(self.tp + self.fn)

	def accuracy(self):
		return float(self.tp + self.tn) / float(self.tp + self.tn + self.fp + self.fn)

#Data Import
dataLocation = "FILL IN DIRECTORY OF DATA"

month = "may"
dataSource = "FILL IN EXACT DIRECTORY LINK OF DATA/Data_2dec_%s.csv" % month

os.chdir(dataLocation)

data = pd.read_table(dataSource, header = 0, sep = ";",error_bad_lines = False, na_values = 'NULL')

#Select predictor variables and target variable
target = data["STATUS"]
identifier = data["customer_key"]

variablesToDrop = [0,1,2,3,4,48,49,53,54,58,59,63,64,68,69]
predictors = data
predictors.drop(predictors.columns[variablesToDrop], axis=1, inplace=True)

y = target
X = predictors

#normalize X
meansX = np.mean(X)
dX = X - meansX
stdX = np.std(X)
sX = dX / stdX
nX = pd.DataFrame(np.ones(len(sX)))
nX = pd.concat([nX, sX], axis=1, ignore_index=True)	#normalized X (predictor variables)

#Machine Learning

#Logistic Regression
logreg = linear_model.LogisticRegression(penalty="l1", C=1e5)
logreg.fit(nX, y)
yHat_lr = logreg.predict(nX)
yProb_lr = logreg.predict_proba(nX)
temp = pd.DataFrame(yProb_lr)
yProb_lr = temp[1]

#Decision Tree
dectree = tree.DecisionTreeClassifier()
dectree.fit(nX, y)
yHat_dt = dectree.predict(nX)
yProb_dt = dectree.predict_proba(nX)
temp = pd.DataFrame(yProb_dt)
yProb_dt = temp[1]

#Random Forest
randfor = rfc()
randfor.fit(nX, y)
yHat_rf = randfor.predict(nX)
yProb_rf = randfor.predict_proba(nX)
temp = pd.DataFrame(yProb_rf)
yProb_rf = temp[1]

#Temp export naar csv
results = pd.DataFrame(y)
results["PRED_LR"] = yHat_lr
results["PRED_DT"] = yHat_dt
results["PRED_RF"] = yHat_rf
results["PROB_LR"] = yProb_lr
results["PROB_DT"] = yProb_dt
results["PROB_RF"] = yProb_rf

results.to_csv("Results.csv", sep=";")

#How to call Evaluation class
ml_models = ["lr", "dt", "rf"]

eval_test = Evaluation(y, yHat_%s) % ml_models[0]

print "True positives: %s" % eval_test.truepositives()
print "Precision: %s" % eval_test.precision()
print "Recall: %s" % eval_test.recall()
print "Accuracy: %s" % eval_test.accuracy()
